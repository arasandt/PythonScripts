{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Demo for the COCO-Text data API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will learn how to load the COCO-Text data using the python API.\n",
    "\n",
    "Let's first import the `coco_text` tool API package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coco_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you have downloaded the annotation file from the website.\n",
    "\n",
    "Once downloaded, you can import the annotations in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:02.032266\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ct = coco_text.COCO_Text('COCO_Text.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets use the API. First, the API offers some basic infos of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://vision.cornell.edu/se3/coco-text/\n",
      "author: COCO-Text group\n",
      "date_created: 2017-03-28\n",
      "description: This is 1.4 version of the 2017 COCO-Text dataset.\n",
      "version: 1.4\n"
     ]
    }
   ],
   "source": [
    "ct.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select annotations and images based on filter criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve some images. We want to get a list of all image ids from the training set, where the image contains at least one text instance that is legilbe and is machine printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = ct.getImgIds(imgIds=ct.train, \n",
    "                    catIds=[('legibility','legible'),('class','machine printed')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go on to the annotations. We want to get a list of all annotation ids from the validation set that are legible, machine printed and have an area between 0 and 200 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = ct.getAnnIds(imgIds=ct.val, \n",
    "                        catIds=[('legibility','legible'),('class','machine printed')], \n",
    "                        areaRng=[0,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Visualize COCOText Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the COCO Text annotations, please make sure to download the COCO Images from the MSCOCO website: http://mscoco.org/dataset/#download "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the images, specify the path to the MSCOCO image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='../../../Desktop/MSCOCO/data'\n",
    "dataType='train2014'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now import some useful tools to visualize the COCO images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the API introduced above, lets select an image that has at least one instance of legible text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images containing at least one instance of legible text\n",
    "imgIds = ct.getImgIds(imgIds=ct.train, \n",
    "                    catIds=[('legibility','legible')])\n",
    "# pick one at random\n",
    "img = ct.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../Desktop/MSCOCO/data/images/train2014/COCO_train2014_000000327572.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-846cbc78e070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/images/%s/%s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/images/%s/%s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\io\\_io.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, as_grey, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ndim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\io\\manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                                (plugin, kind))\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\io\\_plugins\\pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../Desktop/MSCOCO/data/images/train2014/COCO_train2014_000000327572.jpg'"
     ]
    }
   ],
   "source": [
    "I = io.imread('%s/images/%s/%s' %(dataDir,dataType,img['file_name']))\n",
    "print('/images/%s/%s', (dataType,img['file_name']))\n",
    "plt.figure()\n",
    "plt.imshow(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can load and display the text annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load and display text annotations\n",
    "#plt.imshow(I)\n",
    "annIds = ct.getAnnIds(imgIds=img['id'])\n",
    "anns = ct.loadAnns(annIds)\n",
    "ct.showAnns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Demo for the COCO-Text evaluation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will learn how to use the COCO-Text evaluation API to evaluate text detection and recognition results.\n",
    "\n",
    "First, let's import the `coco_text_evaluation` API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coco_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to load our recognition results. For this we can use the `loadRes()` function from the `coco_text` tool.\n",
    "\n",
    "The results have to be saved in the format explained on the website. The '`our_results.json`' file gives an example. Generally, the detections are saved in a json file and form a list of dictionaries like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [{\"image_id\": int,\n",
    "      \"bbox\": [left, top, width, height],\n",
    "      \"utf8_string\": string\"},\n",
    "      {}...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the results like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=0.03s)\n"
     ]
    }
   ],
   "source": [
    "our_results = ct.loadRes('our_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results file contains annotations for images not in the current version of COCO-Text, the loader will notify\n",
    "that some images are skipped and then ignore the respective annotations. This happens for example, if results for the test set are included in the same file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the resutls are loaded, the evalution tool allows to compute the successful detections with the '`getDetections()`' function. The `detection_threshold` parameter defines how closely the bounding boxes need to overlap. The default value is an Intersection over Union (IoU) score of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_detections = coco_evaluation.getDetections(ct, our_results, detection_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detection results comprise three lists: True Positives, False Positives and False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives have a ground truth id and an evaluation id:  {'eval_id': 0, 'gt_id': 1148291}\n",
      "False positives only have an evaluation id:  {'eval_id': 7}\n",
      "True negatives only have a ground truth id:  {'gt_id': 1123384}\n"
     ]
    }
   ],
   "source": [
    "print('True positives have a ground truth id and an evaluation id: ', our_detections['true_positives'][0])\n",
    "print('False positives only have an evaluation id: ', our_detections['false_positives'][0])\n",
    "print('True negatives only have a ground truth id: ', our_detections['false_negatives'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the transcription performance now. For that we ue the '`evaluateTranscription()`' function. And provide our results and detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_endToEnd_results = coco_evaluation.evaluateEndToEnd(ct, our_results, detection_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to see the results. For that we can use the '`printDetailedResults()`' function. The last line can be used to create a table as shown in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our approach\n",
      "\n",
      "Detection\n",
      "Recall\n",
      "legible & machine printed:  0.02\n",
      "legible & handwritten:  0.00\n",
      "legible overall:  0.02\n",
      "illegible & machine printed:  0.00\n",
      "illegible & handwritten:  0.00\n",
      "illegible overall:  0.00\n",
      "total recall:  0.0\n",
      "Precision\n",
      "total precision:  87.50\n",
      "f-score\n",
      "f-score localization:  0.03\n",
      "\n",
      "Transcription\n",
      "accuracy for exact matches:  66.67\n",
      "accuracy for matches with edit distance<=1:  100.00\n",
      "\n",
      "End-to-end\n",
      "recall:  0.02 precision:  58.33\n",
      "End-to-end f-score:  0.03\n",
      "\n",
      "0.02  &  0.00  &  0.00  &  0.00 & 0.0  &  87.50  &  0.03  &  66.67  &  0.02  &  58.33  &  0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coco_evaluation.printDetailedResults(ct,our_detections,our_endToEnd_results,'our approach')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
